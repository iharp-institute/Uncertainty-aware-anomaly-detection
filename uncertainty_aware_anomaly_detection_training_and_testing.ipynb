{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de686ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Lambda, Concatenate, RepeatVector, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Load the Data\n",
    "# -------------------------------\n",
    "test_path = '/home/test_data.csv'\n",
    "train_path = '/home/train_data.csv'\n",
    "cali_path = '/home/cali_data.csv'\n",
    "\n",
    "test_unc_path  = '/home/test_data_unc.csv'\n",
    "train_unc_path = '/home/train_data_unc.csv'\n",
    "cali_unc_path  = '/home/cali_data_unc.csv'\n",
    "\n",
    "# Read the CSVs, setting 'time' as the index column\n",
    "test_data      = pd.read_csv(test_path, index_col='time')\n",
    "train_data     = pd.read_csv(train_path, index_col='time')\n",
    "cali_data      = pd.read_csv(cali_path, index_col='time')\n",
    "\n",
    "test_data_unc  = pd.read_csv(test_unc_path, index_col='time')\n",
    "train_data_unc = pd.read_csv(train_unc_path, index_col='time')\n",
    "cali_data_unc  = pd.read_csv(cali_unc_path, index_col='time')\n",
    "\n",
    "\n",
    "# 2. Scale the Features with MinMaxScaler\n",
    "#    and Keep as DataFrames\n",
    "# -------------------------------\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "train_scaled     = pd.DataFrame(scaler.fit_transform(train_data),      index=train_data.index,      columns=train_data.columns)\n",
    "test_scaled      = pd.DataFrame(scaler.transform(test_data),           index=test_data.index,       columns=test_data.columns)\n",
    "cali_scaled      = pd.DataFrame(scaler.transform(cali_data),           index=cali_data.index,       columns=cali_data.columns)\n",
    "\n",
    "train_unc_scaled = pd.DataFrame(scaler.fit_transform(train_data_unc),  index=train_data_unc.index,  columns=train_data_unc.columns)\n",
    "test_unc_scaled  = pd.DataFrame(scaler.transform(test_data_unc),       index=test_data_unc.index,    columns=test_data_unc.columns)\n",
    "cali_unc_scaled  = pd.DataFrame(scaler.transform(cali_data_unc),       index=cali_data_unc.index,    columns=cali_data_unc.columns)\n",
    "\n",
    "\n",
    "# 3. Determine Optimal Number of Clusters via Elbow and Silhouette\n",
    "# -------------------------------\n",
    "# Compute correlation matrix on the training set\n",
    "corr_mat = train_scaled.corr()\n",
    "\n",
    "# 3a. Elbow Method: compute SSE for cluster counts 1–9\n",
    "sse = []\n",
    "for k in range(1, 10):\n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++', max_iter=2000, random_state=42)\n",
    "    kmeans.fit(corr_mat)\n",
    "    sse.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the elbow curve\n",
    "frame_elbow = pd.DataFrame({'Cluster': range(1, 10), 'SSE': sse})\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(frame_elbow['Cluster'], frame_elbow['SSE'], marker='o')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('SSE (Inertia)')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 3b. Silhouette Method: evaluate silhouette score for k = 2–6\n",
    "silhouette_scores = {}\n",
    "for k in range(2, 7):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    labels = kmeans.fit_predict(corr_mat)\n",
    "    score = silhouette_score(corr_mat, labels)\n",
    "    silhouette_scores[k] = score\n",
    "    print(f\"Silhouette Score for {k} clusters: {score:.4f}\")\n",
    "\n",
    "# Choose the k with the highest silhouette score\n",
    "optimal_clusters = max(silhouette_scores, key=silhouette_scores.get)\n",
    "print(f\"\\nOptimal number of clusters based on silhouette score: {optimal_clusters}\")\n",
    "\n",
    "# 4. Cluster Features on the Correlation Matrix\n",
    "# -------------------------------\n",
    "kmeans_final = KMeans(n_clusters=optimal_clusters, random_state=42)\n",
    "cluster_labels = kmeans_final.fit_predict(corr_mat)\n",
    "\n",
    "def print_clusters(data_df, labels):\n",
    "    cluster_dict = {}\n",
    "    for idx, lbl in enumerate(labels):\n",
    "        feature = data_df.columns[idx]\n",
    "        cluster_dict.setdefault(lbl, []).append(feature)\n",
    "    for lbl, feats in cluster_dict.items():\n",
    "        print(f\"Cluster {lbl}: {', '.join(feats)}\")\n",
    "\n",
    "print(\"\\nFeature clusters on training data:\")\n",
    "print_clusters(train_scaled, cluster_labels)\n",
    "\n",
    "def create_clustered_dataframes(df, labels):\n",
    "    clustered = {}\n",
    "    for lbl in np.unique(labels):\n",
    "        cols_in_lbl = [col for col, lab in zip(df.columns, labels) if lab == lbl]\n",
    "        clustered[lbl] = df[cols_in_lbl]\n",
    "    return clustered\n",
    "\n",
    "# Create clustered DataFrames for train, test, and cali (features only)\n",
    "clustered_train_dfs = create_clustered_dataframes(train_scaled, cluster_labels)\n",
    "clustered_test_dfs  = create_clustered_dataframes(test_scaled, cluster_labels)\n",
    "clustered_cali_dfs  = create_clustered_dataframes(cali_scaled, cluster_labels)\n",
    "\n",
    "# Also prepare clustered DataFrames for uncertainty data (same column structure)\n",
    "clustered_train_unc_dfs = create_clustered_dataframes(train_unc_scaled, cluster_labels)\n",
    "clustered_test_unc_dfs  = create_clustered_dataframes(test_unc_scaled, cluster_labels)\n",
    "clustered_cali_unc_dfs  = create_clustered_dataframes(cali_unc_scaled, cluster_labels)\n",
    "\n",
    "# 5. Create Sequences for Each Cluster\n",
    "# -------------------------------\n",
    "def create_sequences(dataframes_dict, seq_length):\n",
    "    sequences = {}\n",
    "    for lbl, df in dataframes_dict.items():\n",
    "        arr = df.values  # shape: (time_steps, n_features)\n",
    "        xs = []\n",
    "        for i in range(len(arr) - seq_length):\n",
    "            xs.append(arr[i: i + seq_length])\n",
    "        sequences[lbl] = np.stack(xs)  # shape: (n_samples, seq_length, n_features)\n",
    "    return sequences\n",
    "\n",
    "# Sequence length\n",
    "T = 31  \n",
    "\n",
    "# Create sequences for inputs\n",
    "train_sequences     = create_sequences(clustered_train_dfs, T)\n",
    "test_sequences      = create_sequences(clustered_test_dfs, T)\n",
    "cali_sequences      = create_sequences(clustered_cali_dfs, T)\n",
    "\n",
    "# Create sequences for uncertainties (same T) \n",
    "train_unc_sequences = create_sequences(clustered_train_unc_dfs, T)\n",
    "#test_unc_sequences  = create_sequences(clustered_test_unc_dfs, T)\n",
    "#cali_unc_sequences  = create_sequences(clustered_cali_unc_dfs, T)\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Prepare Uncertainty Arrays\n",
    "# -------------------------------\n",
    "'''''\n",
    "def stack_uncertainty(sequences_dict):\n",
    "    \"\"\"\n",
    "    Given a dict of sequences for each cluster, stack them to form a single 3D array:\n",
    "      - Step 1: collect in a list sorted by cluster label\n",
    "      - Step 2: convert to numpy array of shape (n_clusters, n_samples, seq_length, n_features_per_cluster)\n",
    "      - Step 3: rearrange to (n_samples, seq_length, total_n_clusters)\n",
    "    \"\"\"\n",
    "    # List of arrays in ascending order of cluster labels\n",
    "    arr_list = [sequences_dict[lbl] for lbl in sorted(sequences_dict.keys())]\n",
    "    # Convert to array: shape (n_clusters, n_samples, seq_length, n_feats_cluster)\n",
    "    arr_stack = np.stack(arr_list)\n",
    "    # Now we want per-time-step, per-sample uncertainties across clusters:\n",
    "    # Current shape: (n_clusters, n_samples, seq_length, n_feats_cluster)\n",
    "    # But original code treats each cluster's full set as a single dimension. To match that:\n",
    "    #   - First reshape each cluster to (n_samples, seq_length, 1) by collapsing its features\n",
    "    #   - Then stack along last axis.\n",
    "    # Instead, we will compute a single uncertainty per cluster per time step by averaging across features within that cluster:\n",
    "    cluster_counts = [sequences_dict[lbl].shape[-1] for lbl in sorted(sequences_dict.keys())]\n",
    "    n_clusters = len(arr_list)\n",
    "    n_samples = arr_list[0].shape[0]\n",
    "    # Create an empty array with shape (n_samples, seq_length, n_clusters)\n",
    "    stacked = np.zeros((n_samples, T, n_clusters))\n",
    "    for idx, lbl in enumerate(sorted(sequences_dict.keys())):\n",
    "        # Mean across feature dimension within each cluster\n",
    "        # arr_list[idx] has shape (n_samples, seq_length, n_feats_cluster)\n",
    "        stacked[:, :, idx] = arr_list[idx].mean(axis=-1)\n",
    "    return stacked  # shape: (n_samples, seq_length, n_clusters)\n",
    "\n",
    "train_unc_array = stack_uncertainty(train_unc_sequences)\n",
    "test_unc_array  = stack_uncertainty(test_unc_sequences)\n",
    "cali_unc_array  = stack_uncertainty(cali_unc_sequences)\n",
    "'''\n",
    "\n",
    "train_unc_array = [train_unc_sequences [key] for key in sorted(train_unc_sequences .keys())]\n",
    "train_unc_array2 =  np.array(train_unc_array)\n",
    "train_unc_array = train_unc_array2.reshape(train_unc_array2.shape[1],train_unc_array2.shape[2],train_unc_array2.shape[0])\n",
    "print(\"Uncertainty array shape:\", train_unc_array.shape)\n",
    "\n",
    "\n",
    "# 7. Define the VAE with Monte Carlo Dropout\n",
    "# -------------------------------\n",
    "class MCDropout(tf.keras.layers.Dropout):\n",
    "    \"\"\"Monte Carlo Dropout that remains active at inference time.\"\"\"\n",
    "    def call(self, inputs, training=None):\n",
    "        return super().call(inputs, training=training or K.learning_phase())\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim   = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "def vae_model(input_sequences_list, kl_weight=0.75, dropout_rate=0.2):\n",
    "    inputs   = []\n",
    "    encoders = []\n",
    "    # Sum of all features across clusters for the output dimension\n",
    "    total_features = sum([X.shape[2] for X in input_sequences_list])\n",
    "\n",
    "    # Build separate encoder branches for each cluster input\n",
    "    for X in input_sequences_list:\n",
    "        inp = Input(shape=(X.shape[1], X.shape[2]))  # (seq_length, n_features_cluster)\n",
    "        h = LSTM(64, activation='relu', return_sequences=True)(inp)\n",
    "        h = MCDropout(dropout_rate)(h)\n",
    "        h = LSTM(32, activation='relu', return_sequences=True)(h)\n",
    "        h = MCDropout(dropout_rate * 0.2)(h)\n",
    "        h = LSTM(16, activation='relu', return_sequences=False)(h)\n",
    "        z_mean   = Dense(16)(h)\n",
    "        z_log_var= Dense(16)(h)\n",
    "        z        = Lambda(sampling, output_shape=(16,))([z_mean, z_log_var])\n",
    "        inputs.append(inp)\n",
    "        encoders.append(z)\n",
    "\n",
    "    # Concatenate latent vectors from all clusters\n",
    "    concat = Concatenate(axis=-1)(encoders)\n",
    "    # Decoder: repeat the concatenated latent vector for each time step\n",
    "    dec = RepeatVector(input_sequences_list[0].shape[1])(concat)\n",
    "    dec = LSTM(16, activation='relu', return_sequences=True)(dec)\n",
    "    dec = LSTM(32, activation='relu', return_sequences=True)(dec)\n",
    "    dec = LSTM(64, activation='relu', return_sequences=True)(dec)\n",
    "    output = TimeDistributed(Dense(total_features))(dec)\n",
    "\n",
    "    vae = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "    # Add KL divergence loss\n",
    "    kl_loss = -0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    vae.add_loss(kl_weight * K.mean(kl_loss))\n",
    "\n",
    "    # Custom weighted MSE loss: accounts for per-sample uncertainty\n",
    "    def weighted_mse_loss():\n",
    "        def loss(y_true, y_pred):\n",
    "            true_target     = y_true[:, :, : total_features]\n",
    "            sample_uncer    = y_true[:, :, total_features:]\n",
    "            weights         = 1.0 / (1.0 + sample_uncer)\n",
    "            mse             = K.square(y_pred - true_target)\n",
    "            weighted_mse    = mse * weights\n",
    "            return K.mean(weighted_mse)\n",
    "        return loss\n",
    "\n",
    "    # Learning rate schedule\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=5e-4,\n",
    "        decay_steps=1000,\n",
    "        decay_rate=0.85\n",
    "    )\n",
    "    vae.compile(optimizer=Adam(learning_rate=lr_schedule), loss=weighted_mse_loss())\n",
    "    return vae\n",
    "\n",
    "def mc_dropout_predictions(model, X_list, n_simulations=100):\n",
    "    \"\"\"\n",
    "    Perform multiple stochastic forward passes (Monte Carlo Dropout) to estimate mean\n",
    "    and standard deviation of predictions.\n",
    "    \"\"\"\n",
    "    preds = np.array([model.predict(X_list, batch_size=256, verbose=0) for _ in range(n_simulations)])\n",
    "    mean_pred = preds.mean(axis=0)\n",
    "    std_pred  = preds.std(axis=0)\n",
    "    return mean_pred, std_pred\n",
    "\n",
    "# 8. Prepare Data for Model Training\n",
    "# -------------------------------\n",
    "train_input_list = [train_sequences[lbl] for lbl in sorted(train_sequences.keys())]\n",
    "test_input_list  = [test_sequences[lbl]  for lbl in sorted(test_sequences.keys())]\n",
    "cali_input_list  = [cali_sequences[lbl]  for lbl in sorted(cali_sequences.keys())]\n",
    "\n",
    "for idx, X in enumerate(train_input_list):\n",
    "    print(f\"Cluster {idx} train input shape: {X.shape}  # (n_samples, seq_length, n_feats)\")\n",
    "\n",
    "# Build the VAE model\n",
    "vae = vae_model(train_input_list, kl_weight=0.75, dropout_rate=0.2)\n",
    "\n",
    "true_data = np.concatenate(train_input_list, axis=-1)  # shape: (n_samples, seq_length, total_features)\n",
    "unc_data  = train_unc_array.copy  \n",
    "\n",
    "combined_target = np.concatenate([true_data, unc_data], axis=-1)\n",
    "print(f\"Combined training target shape: {combined_target.shape}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 9. Train the VAE\n",
    "# -------------------------------\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=1e-3,\n",
    "    patience=10,\n",
    "    verbose=1,\n",
    "    mode='auto',\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = vae.fit(\n",
    "    train_input_list,\n",
    "    combined_target,\n",
    "    epochs=100,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stop],\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# 10. Evaluate on Test Set with MC Dropout\n",
    "# -------------------------------\n",
    "mean_pred_test, std_pred_test = mc_dropout_predictions(vae, test_input_list, n_simulations=100)\n",
    "\n",
    "def sequences_to_original(X_seq, seq_length):\n",
    "    n_samples, L, n_feats = X_seq.shape\n",
    "    reconstructed = np.zeros((n_samples + seq_length, n_feats))\n",
    "    for i in range(n_samples):\n",
    "        reconstructed[i] = X_seq[i, 0]\n",
    "    reconstructed[n_samples:] = X_seq[-1, :, :]\n",
    "    return reconstructed\n",
    "\n",
    "test_pred_recon        = sequences_to_original(mean_pred_test, T)\n",
    "test_unc_recon         = sequences_to_original(std_pred_test, T)\n",
    "\n",
    "def reconstruct_actual(clustered_dfs_dict):\n",
    "    dfs = [clustered_dfs_dict[lbl] for lbl in sorted(clustered_dfs_dict.keys())]\n",
    "    concat_df = pd.concat(dfs, axis=1)\n",
    "    return concat_df.values\n",
    "\n",
    "actual_test_values = reconstruct_actual(clustered_test_dfs)  \n",
    "n_samples_test = mean_pred_test.shape[0]\n",
    "mse_per_sample_test = np.mean((test_pred_recon[:n_samples_test] - actual_test_values[:n_samples_test])**2, axis=1)\n",
    "uncertainty_per_sample_test = np.mean(test_unc_recon[:n_samples_test], axis=1)\n",
    "\n",
    "\n",
    "results_test_df = pd.DataFrame({\n",
    "    'mse_per_sample': mse_per_sample_test,\n",
    "    'pred_uncertainty': uncertainty_per_sample_test\n",
    "})\n",
    "results_test_df.to_csv('/home/test_results.csv', index=False)\n",
    "\n",
    "# Save the trained VAE model\n",
    "vae.save('/home/model')\n",
    "\n",
    "# 11. Calibrate on Calibration Set with MC Dropout\n",
    "# -------------------------------\n",
    "mean_pred_cali, std_pred_cali = mc_dropout_predictions(vae, cali_input_list, n_simulations=50)\n",
    "cali_pred_recon = sequences_to_original(mean_pred_cali, T)\n",
    "cali_unc_recon  = sequences_to_original(std_pred_cali, T)\n",
    "\n",
    "actual_cali_values = reconstruct_actual(clustered_cali_dfs)\n",
    "\n",
    "n_samples_cali = mean_pred_cali.shape[0]\n",
    "mse_per_sample_cali = np.mean((cali_pred_recon[:n_samples_cali] - actual_cali_values[:n_samples_cali])**2, axis=1)\n",
    "uncertainty_per_sample_cali = np.mean(cali_unc_recon[:n_samples_cali], axis=1)\n",
    "\n",
    "results_cali_df = pd.DataFrame({\n",
    "    'mse_per_sample': mse_per_sample_cali,\n",
    "    'pred_uncertainty': uncertainty_per_sample_cali\n",
    "})\n",
    "results_cali_df.to_csv('/home/cali_results.csv', index=False)\n",
    "\n",
    "print(\"\\Completed. Results saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a9eaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "\n",
    "def detect_pot_gdp(\n",
    "    df: pd.DataFrame,\n",
    "    column: str = 'run_1',\n",
    "    threshold_pct: float = 95.0,\n",
    "    anomaly_p: float = 0.5,\n",
    "    output_col: str = 'pot_gdp'\n",
    ") -> pd.DataFrame:\n",
    "    df2 = df.copy()\n",
    "    data = df2[column].dropna().values\n",
    "    thresh = np.percentile(data, threshold_pct)\n",
    "    peaks = data[data > thresh]\n",
    "    shape, loc, scale = stats.genpareto.fit(peaks)\n",
    "    all_vals = df2[column].values\n",
    "    tail_p = 1.0 - stats.genpareto.cdf(all_vals, shape, loc, scale)\n",
    "    df2[output_col] = (tail_p < anomaly_p).astype(int)\n",
    "    # 8) (optional) store metadata in attrs\n",
    "    df2[output_col].attrs = {\n",
    "        'gpd_threshold':    float(thresh),\n",
    "        'gpd_shape':        float(shape),\n",
    "        'gpd_loc':          float(loc),\n",
    "        'gpd_scale':        float(scale),\n",
    "        'anomaly_p_level':  float(anomaly_p)\n",
    "    }\n",
    "    return df2\n",
    "\n",
    "era = detect_pot_gdp(era, column='run_1', threshold_pct=95, anomaly_p=0.5, output_col='predicted_anomaly')\n",
    "mar = detect_pot_gdp(mar, column='run_1', threshold_pct=95, anomaly_p=0.5, output_col='predicted_anomaly')\n",
    "gemb = detect_pot_gdp(gemb, column='run_1', threshold_pct=95, anomaly_p=0.5, output_col='predicted_anomaly')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
