{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d041f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "## PRE-PROCESSING ERA5 DATA\n",
    "def process_era_nc(\n",
    "    input_dir: str,\n",
    "    output_csv: str,\n",
    "    file_prefix: str,\n",
    "    resample_freq: str\n",
    ") -> pd.DataFrame:\n",
    "    \n",
    "    # 1) collect file paths\n",
    "    file_paths = [\n",
    "        os.path.join(input_dir, f)\n",
    "        for f in os.listdir(input_dir)\n",
    "        if f.endswith(\".nc\") and f.startswith(file_prefix)\n",
    "    ]\n",
    "\n",
    "    # 2) open and combine\n",
    "    ds = xr.open_mfdataset(\n",
    "        file_paths,\n",
    "        combine=\"by_coords\",\n",
    "        parallel=True,\n",
    "        decode_times=True,\n",
    "    )\n",
    "\n",
    "    # 3) to pandas DataFrame\n",
    "    df = ds.to_dataframe().reset_index()\n",
    "\n",
    "    # 4) rename coords and parse time\n",
    "    df = df.rename(columns={\n",
    "        \"valid_time\": \"time\",\n",
    "        \"latitude\":   \"lat\",\n",
    "        \"longitude\":  \"lon\"\n",
    "    })\n",
    "    df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
    "    df = df.set_index(\"time\")\n",
    "\n",
    "    # 5) group by lat/lon, resample and mean\n",
    "    df = df.groupby([\"lat\", \"lon\"]).resample(resample_freq).mean()\n",
    "\n",
    "    # 6) compute derived fields\n",
    "    df[\"ssrdas\"] = df[\"ssrd\"] * (1 - df[\"asn\"])\n",
    "    df[\"tw10\"]   = np.hypot(df[\"u10\"], df[\"v10\"])\n",
    "\n",
    "    # 7) cleanup\n",
    "    df = df.drop(columns=[\"lat\", \"lon\", \"number\", \"expver\"], errors=\"ignore\")\n",
    "    df = df.reset_index()\n",
    "    df = df.sort_values(by=[\"lon\", \"lat\"], ascending=[True, False])\n",
    "\n",
    "    # 8) write out\n",
    "    df.to_csv(output_csv, index=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "era = process_era_nc(\n",
    "    input_dir=\"Dataset/\",\n",
    "    output_csv=\"/home/ERA_NO.csv\",\n",
    "    file_prefix=\"data_stream-oper_stepType-\",\n",
    "    resample_freq=\"5D\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6695b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def filter_and_merge_gemb(\n",
    "    albedo_csv: str,\n",
    "    melt_csv: str,\n",
    "    lat_range: tuple = (68.0, 72.0),\n",
    "    lon_range: tuple = (-51.5, -47.5),\n",
    "    start_year: str = '1981',\n",
    "    end_year: str   = '2020',\n",
    "    output_csv: str = None\n",
    ") -> pd.DataFrame:\n",
    "    \n",
    "    # 1) Load\n",
    "    albedo = pd.read_csv(albedo_csv)\n",
    "    melt   = pd.read_csv(melt_csv)\n",
    "\n",
    "    # 2) Spatial filtering\n",
    "    lat_min, lat_max = lat_range\n",
    "    lon_min, lon_max = lon_range\n",
    "\n",
    "    albedo_f = albedo[\n",
    "        (albedo['lat'] >= lat_min) & (albedo['lat'] <= lat_max) &\n",
    "        (albedo['lon'] >= lon_min) & (albedo['lon'] <= lon_max)\n",
    "    ].copy()\n",
    "\n",
    "    melt_f = melt[\n",
    "        (melt['lat'] >= lat_min) & (melt['lat'] <= lat_max) &\n",
    "        (melt['lon'] >= lon_min) & (melt['lon'] <= lon_max)\n",
    "    ].copy()\n",
    "\n",
    "    # 3) Time slice both\n",
    "    for df in (albedo_f, melt_f):\n",
    "        df['time'] = pd.to_datetime(df['time'])\n",
    "        df.set_index('time', inplace=True)\n",
    "        df.sort_index(inplace=True)\n",
    "        df = df.loc[start_year:end_year]\n",
    "        df.reset_index(inplace=True)\n",
    "        # assign back\n",
    "        if df is albedo_f:\n",
    "            albedo_f = df\n",
    "        else:\n",
    "            melt_f = df\n",
    "\n",
    "    # 4) Sort spatially\n",
    "    albedo_f.sort_values(by=['lon','lat'], ascending=[True, False], inplace=True)\n",
    "    melt_f.sort_values(by=['lon','lat'],  ascending=[True, False], inplace=True)\n",
    "\n",
    "    # 5) Merge on time, lon, lat\n",
    "    merged = pd.merge(\n",
    "        albedo_f,\n",
    "        melt_f,\n",
    "        on=['time','lon','lat'],\n",
    "        how='inner'\n",
    "    )\n",
    "\n",
    "    # 6) Build uniform 5-day index from 1981-01-01 to 2020-12-30, drop Feb 29\n",
    "    base = pd.date_range('1981-01-01', '2020-12-30', freq='5D')\n",
    "    mask = ~((base.month == 2) & (base.day == 29))\n",
    "    base = base[mask]\n",
    "\n",
    "    # 7) Repeat that cycle for each grid cell\n",
    "    n_cells = merged['lat'].nunique() * merged['lon'].nunique()\n",
    "    repeated_index = np.tile(base.values, n_cells)\n",
    "    repeated_index = pd.to_datetime(repeated_index)\n",
    "\n",
    "    # 8) Assign the new time index and filter to Mayâ€“Sep\n",
    "    merged = merged.drop(columns=['time'])\n",
    "    merged['time'] = repeated_index\n",
    "    merged.set_index('time', inplace=True)\n",
    "    merged = merged[merged.index.month.isin([5, 6, 7, 8, 9])]\n",
    "\n",
    "    # 9) Output\n",
    "    if output_csv:\n",
    "        merged.to_csv(output_csv, index=False)\n",
    "\n",
    "    return merged\n",
    "\n",
    "merged = filter_and_merge_gemb(\n",
    "    albedo_csv='/home/GEMB/albedo.csv',\n",
    "    melt_csv=  '/home/GEMB/melt.csv',\n",
    "    lat_range=(79.5, 82.5),\n",
    "    lon_range=(-60.5, -24.5),\n",
    "    start_year='1981',\n",
    "    end_year='2020',\n",
    "    output_csv='/home/GEMB_NO.csv'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe372609",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PRE-PROCESSING MAR DATA\n",
    "def process_mar(\n",
    "    input_csv: str,\n",
    "    output_csv: str,\n",
    "    lon_range: tuple,\n",
    "    lat_range: tuple,\n",
    "    months: list\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    # Load data\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # Spatial filter\n",
    "    lon_min, lon_max = lon_range\n",
    "    lat_min, lat_max = lat_range\n",
    "    df = df[\n",
    "        (df['lon'] >= lon_min) & (df['lon'] <= lon_max) &\n",
    "        (df['lat'] >= lat_min) & (df['lat'] <= lat_max)\n",
    "    ].copy()\n",
    "\n",
    "    # Time filter\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df = df.set_index('time')\n",
    "    df = df[df.index.month.isin(months)]\n",
    "\n",
    "    # Compute SWDAS\n",
    "    df['SWDAS'] = df['SWD'] * (1.0 - df['AL2'])\n",
    "\n",
    "    # Convert ME from mm to m\n",
    "    df['ME'] = df['ME'] / 1000.0\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(output_csv)\n",
    "\n",
    "    return df\n",
    "\n",
    "mar = process_mar(\n",
    "    input_csv='/home/mar_data_1981_2020.csv',\n",
    "    output_csv='/home/MAR_NO.csv',\n",
    "    lat_range=(79.5, 82.5),\n",
    "    lon_range=(-60.5, -24.5),\n",
    "    months=[5, 6, 7, 8, 9]\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
